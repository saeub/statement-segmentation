{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference for masked tokens with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "from data import Sentence, load_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-german-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-german-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-german-cased\")\n",
    "\n",
    "MASK_TOKEN_ID = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e2af7d64da4a0f86b459986f1e4744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "INPATH = Path(__name__).absolute().parent.parent / \"data/train.csv\"\n",
    "OUTPATH = Path(__name__).absolute().parent.parent / \"data/train_augmented.csv\"\n",
    "\n",
    "def replace_nouns(sentence_row: pd.Series) -> List[pd.Series]:\n",
    "\n",
    "    sentence = Sentence.from_row(sentence_row)\n",
    "\n",
    "    noun_indices = []\n",
    "    for i, token in enumerate(sentence.tokens):\n",
    "\n",
    "        try:\n",
    "            a = token[0]\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        if i > 0 and token[0].isupper():# and token[-1].isalpha():\n",
    "            noun_indices.append(i)\n",
    "\n",
    "    pseudo_rows = []\n",
    "\n",
    "    for i in noun_indices:\n",
    "        new_sentence_list = deepcopy(sentence.tokens)\n",
    "        new_sentence_list[i] = \"[MASK]\"\n",
    "\n",
    "        if not sentence.tokens[i][-1].isalpha():\n",
    "            new_sentence_list.insert(i+1, sentence.tokens[i][-1])\n",
    "            suffixed_by_noalpha = True\n",
    "        else:\n",
    "            suffixed_by_noalpha = False\n",
    "        \n",
    "        # Get predictions\n",
    "        new_sentence = \" \".join(new_sentence_list)\n",
    "        inputs = tokenizer(new_sentence, return_tensors=\"pt\")\n",
    "        mask_token_index = torch.where(inputs[\"input_ids\"] == MASK_TOKEN_ID)[1]\n",
    "        token_logits = model(**inputs).logits\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        top_tokens = torch.topk(mask_token_logits, 50, dim=1).indices[0].tolist()\n",
    "        \n",
    "        n = len(pseudo_rows)\n",
    "        for token_id in top_tokens:\n",
    "            if len(pseudo_rows) - n > 4:\n",
    "                break\n",
    "\n",
    "            token = tokenizer.decode([token_id])\n",
    "\n",
    "            if token[0].isupper() and token != sentence.tokens[i]:\n",
    "                \n",
    "                new_sentence_filled = deepcopy(new_sentence_list)\n",
    "                new_sentence_filled[i] = token\n",
    "\n",
    "                if suffixed_by_noalpha:\n",
    "                    new_sentence_filled.pop(i+1)\n",
    "                    new_sentence_filled[i] = new_sentence_filled[i] + sentence.tokens[i][-1]\n",
    "\n",
    "                new_sentence = \" \".join(new_sentence_filled)\n",
    "\n",
    "                pseudo_row = pd.Series({\n",
    "                    \"sent-id\": replace_nouns.running_idx,\n",
    "                    \"topic\": sentence.topic,\n",
    "                    \"phrase\": new_sentence,\n",
    "                    \"phrase_number\": f\"AUG-{sentence_row['sent-id']}\",\n",
    "                    \"genre\": sentence_row[\"genre\"],\n",
    "                    \"timestamp\": sentence_row[\"timestamp\"],\n",
    "                    \"user\": \"dataaugmenter\",\n",
    "                    \"phrase_tokenized\": \" \".join([f\"{i}:={token}\" for i, token in enumerate(new_sentence_filled)]),\n",
    "                    \"statement_spans\": sentence.statement_spans.__repr__(),\n",
    "                    \"num_statements\": len(sentence.statement_spans),\n",
    "                })\n",
    "\n",
    "                pseudo_rows.append(pseudo_row)\n",
    "\n",
    "                replace_nouns.running_idx += 1\n",
    "\n",
    "    return pseudo_rows\n",
    "replace_nouns.running_idx = 0\n",
    "\n",
    "df = pd.read_csv(INPATH)\n",
    "df_augmented = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    new_rows = replace_nouns(row)\n",
    "    # Add the new rows to the augmented dataframe\n",
    "    for new_row in new_rows:\n",
    "        df_augmented = pd.concat([df_augmented, pd.DataFrame([new_row])])\n",
    "\n",
    "df_augmented.to_csv(OUTPATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_mathesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
